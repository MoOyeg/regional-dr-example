---
# =============================================================================
# NetObserv Cleanup
# =============================================================================
# Play 1: Remove ACM Policy and hub-side resources (hub only)
# Play 2: Clean up per-cluster resources and S3 buckets (each netobserv cluster)
# =============================================================================

# =============================================================================
# Play 1: Remove ACM Policy and hub-side resources
# =============================================================================
- name: "Play 1: Remove ACM Policy and hub-side resources"
  hosts: openshift_clusters
  gather_facts: false

  vars:
    artifacts_dir: "{{ playbook_dir }}/artifacts"
    policy_name: "netobserv-policy"
    policy_namespace: "netobserv-policy"

  pre_tasks:
    - name: Skip non-hub clusters
      meta: end_host
      when: cluster_role | default('spoke') != 'hub'

    - name: Validate kubeconfig exists
      stat:
        path: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      register: kubeconfig_stat
      delegate_to: localhost

    - name: End host if kubeconfig missing
      meta: end_host
      when: not kubeconfig_stat.stat.exists

    - name: Build list of netobserv clusters
      set_fact:
        netobserv_clusters: "{{ groups['openshift_clusters'] |
          map('extract', hostvars) |
          selectattr('netobserv', 'defined') |
          selectattr('netobserv', 'equalto', true) |
          map(attribute='cluster_name') |
          list }}"

    - name: Build managedcluster name mapping
      set_fact:
        managedcluster_names: >-
          {
          {% for spoke_cluster in netobserv_clusters %}
          {% set spoke_vars = hostvars[spoke_cluster] %}
          {% if spoke_vars.managedcluster_name is defined %}
          "{{ spoke_cluster }}": "{{ spoke_vars.managedcluster_name }}"
          {% elif spoke_vars.cluster_role | default('spoke') == 'hub' %}
          "{{ spoke_cluster }}": "local-cluster"
          {% else %}
          "{{ spoke_cluster }}": "{{ spoke_cluster }}"
          {% endif %}
          {% if not loop.last %},{% endif %}
          {% endfor %}
          }

  tasks:
    - name: Remove netobserv=true label from managed clusters
      shell: |
        oc label managedcluster {{ managedcluster_names[item] }} netobserv- --overwrite
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      loop: "{{ netobserv_clusters }}"
      changed_when: false
      ignore_errors: true

    - name: Delete policy namespace (cascades Policy, Placement, PlacementBinding, hub secrets)
      shell: |
        oc delete namespace {{ policy_namespace }} --ignore-not-found --timeout=120s
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    - name: Wait for policy namespace deletion
      shell: |
        oc get namespace {{ policy_namespace }} 2>&1 | grep -q "not found" && echo "deleted" || echo "pending"
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      register: ns_delete_check
      until: '"deleted" in ns_delete_check.stdout'
      retries: 20
      delay: 10
      changed_when: false
      ignore_errors: true

    - name: Display hub cleanup result
      debug:
        msg: |
          Hub cleanup complete:
          - Removed netobserv=true labels from: {{ netobserv_clusters | join(', ') }}
          - Deleted namespace: {{ policy_namespace }}
            (cascaded: Policy, Placement, PlacementBinding, hub-side S3 secrets)

# =============================================================================
# Play 2: Clean up per-cluster resources and S3 buckets
# =============================================================================
- name: "Play 2: Clean up per-cluster resources and S3 buckets"
  hosts: openshift_clusters
  gather_facts: false

  vars:
    artifacts_dir: "{{ playbook_dir }}/artifacts"
    loki_namespace: "openshift-logging"

  pre_tasks:
    - name: Skip cluster if netobserv is not enabled
      meta: end_host
      when: netobserv is not defined or not netobserv | bool

    - name: Verify kubeconfig exists
      stat:
        path: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      register: kubeconfig_stat
      delegate_to: localhost

    - name: End host if kubeconfig missing
      meta: end_host
      when: not kubeconfig_stat.stat.exists

    - name: Retrieve AWS credentials
      set_fact:
        aws_access_key: "{{ lookup('env', 'AWS_ACCESS_KEY_ID_' + (aws_credential_set | string)) }}"
        aws_secret_key: "{{ lookup('env', 'AWS_SECRET_ACCESS_KEY_' + (aws_credential_set | string)) }}"
        aws_deploy_region: "{{ lookup('env', 'AWS_REGION_' + (aws_credential_set | string)) | default(aws_region, true) }}"

  tasks:
    # =========================================================================
    # Delete FlowCollector (cluster-scoped)
    # =========================================================================
    - name: Delete FlowCollector
      shell: |
        oc delete flowcollector cluster --ignore-not-found --timeout=60s
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    - name: Wait for FlowCollector deletion
      shell: |
        oc get flowcollector cluster 2>&1 | grep -q "not found" && echo "deleted" || echo "pending"
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      register: fc_delete_check
      until: '"deleted" in fc_delete_check.stdout'
      retries: 10
      delay: 5
      changed_when: false
      ignore_errors: true

    # =========================================================================
    # Delete LokiStack
    # =========================================================================
    - name: Delete LokiStack
      shell: |
        oc delete lokistack loki -n {{ loki_namespace }} --ignore-not-found --timeout=120s
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    - name: Wait for LokiStack deletion
      shell: |
        oc get lokistack loki -n {{ loki_namespace }} 2>&1 | grep -q "not found" && echo "deleted" || echo "pending"
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      register: ls_delete_check
      until: '"deleted" in ls_delete_check.stdout'
      retries: 20
      delay: 10
      changed_when: false
      ignore_errors: true

    # =========================================================================
    # Delete Loki S3 credentials secret
    # =========================================================================
    - name: Delete Loki S3 credentials secret
      shell: |
        oc delete secret loki-s3-credentials -n {{ loki_namespace }} --ignore-not-found
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    # =========================================================================
    # Operator cleanup (safety net if ACM policy removal didn't cascade)
    # =========================================================================
    - name: Delete NetObserv Subscription
      shell: |
        oc delete subscription --all -n openshift-netobserv-operator --ignore-not-found
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    - name: Delete NetObserv CSVs
      shell: |
        oc delete csv --all -n openshift-netobserv-operator --ignore-not-found
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    - name: Delete Loki Subscription
      shell: |
        oc delete subscription --all -n openshift-operators-redhat --ignore-not-found
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    - name: Delete Loki CSVs
      shell: |
        oc delete csv -n openshift-operators-redhat -l operators.coreos.com/loki-operator.openshift-operators-redhat --ignore-not-found
      environment:
        KUBECONFIG: "{{ artifacts_dir }}/{{ cluster_name }}/kubeconfig"
      delegate_to: localhost
      ignore_errors: true

    # =========================================================================
    # Delete S3 bucket
    # =========================================================================
    - name: Check if S3 bucket name was saved
      stat:
        path: "{{ artifacts_dir }}/{{ cluster_name }}/s3-bucket-name"
      register: s3_bucket_file
      delegate_to: localhost

    - name: Read saved S3 bucket name
      shell: cat {{ artifacts_dir }}/{{ cluster_name }}/s3-bucket-name
      delegate_to: localhost
      register: saved_s3_bucket
      changed_when: false
      when: s3_bucket_file.stat.exists

    - name: Empty S3 bucket (current objects)
      shell: |
        AWS_ACCESS_KEY_ID="{{ aws_access_key }}" \
        AWS_SECRET_ACCESS_KEY="{{ aws_secret_key }}" \
        aws s3 rm s3://{{ saved_s3_bucket.stdout }} --recursive --region {{ aws_deploy_region }}
      delegate_to: localhost
      ignore_errors: true
      when: s3_bucket_file.stat.exists

    - name: Delete all object versions from versioned S3 bucket
      shell: |
        AWS_ACCESS_KEY_ID="{{ aws_access_key }}" \
        AWS_SECRET_ACCESS_KEY="{{ aws_secret_key }}" \
        aws s3api list-object-versions \
          --bucket "{{ saved_s3_bucket.stdout }}" \
          --region "{{ aws_deploy_region }}" \
          --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
          --output json 2>/dev/null | \
        AWS_ACCESS_KEY_ID="{{ aws_access_key }}" \
        AWS_SECRET_ACCESS_KEY="{{ aws_secret_key }}" \
        aws s3api delete-objects \
          --bucket "{{ saved_s3_bucket.stdout }}" \
          --region "{{ aws_deploy_region }}" \
          --delete file:///dev/stdin 2>/dev/null || true
      delegate_to: localhost
      ignore_errors: true
      when: s3_bucket_file.stat.exists

    - name: Delete all delete markers from versioned S3 bucket
      shell: |
        AWS_ACCESS_KEY_ID="{{ aws_access_key }}" \
        AWS_SECRET_ACCESS_KEY="{{ aws_secret_key }}" \
        aws s3api list-object-versions \
          --bucket "{{ saved_s3_bucket.stdout }}" \
          --region "{{ aws_deploy_region }}" \
          --query '{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' \
          --output json 2>/dev/null | \
        AWS_ACCESS_KEY_ID="{{ aws_access_key }}" \
        AWS_SECRET_ACCESS_KEY="{{ aws_secret_key }}" \
        aws s3api delete-objects \
          --bucket "{{ saved_s3_bucket.stdout }}" \
          --region "{{ aws_deploy_region }}" \
          --delete file:///dev/stdin 2>/dev/null || true
      delegate_to: localhost
      ignore_errors: true
      when: s3_bucket_file.stat.exists

    - name: Delete S3 bucket
      shell: |
        AWS_ACCESS_KEY_ID="{{ aws_access_key }}" \
        AWS_SECRET_ACCESS_KEY="{{ aws_secret_key }}" \
        aws s3api delete-bucket \
          --bucket "{{ saved_s3_bucket.stdout }}" \
          --region "{{ aws_deploy_region }}" 2>&1 || echo "Bucket already deleted"
      delegate_to: localhost
      ignore_errors: true
      when: s3_bucket_file.stat.exists

    - name: Remove S3 bucket name from artifacts
      file:
        path: "{{ artifacts_dir }}/{{ cluster_name }}/s3-bucket-name"
        state: absent
      delegate_to: localhost
      when: s3_bucket_file.stat.exists

    - name: Display cleanup complete
      debug:
        msg: |
          NetObserv Cleanup Complete - {{ cluster_name }}
          Removed: FlowCollector, LokiStack, S3 credentials, operators
          S3 bucket: {{ saved_s3_bucket.stdout | default('not found') if s3_bucket_file.stat.exists else 'not found' }}
